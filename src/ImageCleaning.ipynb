{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Cleaning\n",
    "\n",
    "Writing image analysis algorithm for microscope images can become very challening due to the noise present in images. There are multiple sources for this noise. Many times the sample labelling is weak and sensor gain needs to be pushed up recover image signal. Other time, presense of a bright flourescence in one channel bleeds over to another and washes the other image. Or to capture images quickly, exposure time is reduced. \n",
    "\n",
    "Noise in itself isn't a big problem if we are not performing quantitative analysis from the images. Since I have to do that, I have seen that writing robust algorithms which work in presense of large image noise is often very challenging. So here I explore if I can use auto-encoder to clean microscope images and make my life easier while I design image processing algorithm.\n",
    "\n",
    "My basic idea in this project is to use a deep learning model to clean microscope images. My deep learning model is based on convolutional autoencoders. To train the model I synthetically generate clean and noisy microscopic images. In image generation I try to synthesis features typically seen microscopic images. Then I distort these clean images considering theoritical optical model of the microscope and add noise to it. \n",
    "\n",
    "After training the model, I noticed that I was able to recover back clean images from very noisy images. Here is a sneak peak of some of the results. I am pretty amazed by how well the model was able to restore the noisy images back to the original clean image. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import tensorflow as tf\n",
    "import scipy.io as sio\n",
    "from scipy import ndimage\n",
    "import time\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# My support code\n",
    "import image_generation as imgen\n",
    "from imshow3D import imshow3D\n",
    "from model import ImageCleaner\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Image Generation\n",
    "\n",
    "\n",
    "### Theoretical true clean image\n",
    "The first step in image generation is creating a general biological sample. This in itself is a very difficult task. How do you best represnt features of fluroscently labelled biological sample. \n",
    "\n",
    "I create sudo-biological sample by creating a clean 3D image with randomly seeded blobs of various size and shapes. The blobs are generated by a gaussian distribution of varying sigma in each direction. For example, here I will create a 64x64x64 volume and visualize in slice by slice from my custom 3D imageviewer based on ipywidgets. \n",
    "\n",
    "This is my theoretical true clean image, without any optical distortion or image noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizeI = np.array([64,64,64])\n",
    "sigma_pts = {140:1.5*np.array([1,1,1]), \n",
    "            70:1.75*np.array([1,1,1]), \n",
    "            35:2*np.array([1,1,1])} # Number of particle and corresponding mean sigma\n",
    "img = np.zeros(sizeI)\n",
    "\n",
    "for nPts, sigma in sigma_pts.items():\n",
    "    # Vary number of points and sigma\n",
    "    nPts = np.round(np.random.poisson(nPts))\n",
    "    pts = imgen.random_seed_locations(nPts, sizeI)\n",
    "    sigma = (sigma + np.random.normal(0, 0.2*sigma[0], \n",
    "                        size=(nPts,3)))\n",
    "    sigma = np.absolute(sigma)\n",
    "    \n",
    "    # Generate img\n",
    "    img = np.maximum(imgen.seedBeadsN(pts, sizeI, sigma), img)\n",
    "\n",
    "# Temporary visualize for Github notebook rendering\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.imshow(img[:,:,0], cmap='gray', vmin=0, vmax=1)\n",
    "plt.colorbar()\n",
    "\n",
    "# Visuzlize using this on local computer. Commenting this because Github \n",
    "# does not show ipywidgets\n",
    "\n",
    "# imshow3D(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blur Image with a PSF\n",
    "\n",
    "Every optical system has its own point spread function (psf) which blurs the image captured by it. In this case, we simulate a psf from Born & Wolf 3D optical model, and blur the image with it. \n",
    "\n",
    "We can see that because of the psf, images lose their sharpness and it becomes more challending to segment neighboring blobs. This is also a true problem faced while segmenting neigboring small particles in microscope images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psf = sio.loadmat('../data/psf.mat')\n",
    "psf = psf['psf']\n",
    "psf = psf/np.sum(psf)\n",
    "\n",
    "\n",
    "blurred_img = ndimage.convolve(img, psf, mode='constant', cval=0.0)\n",
    "\n",
    "# Show the two images side-by side to visualize the original image (left) vs \n",
    "# blurring effect (right)\n",
    "\n",
    "# Temporary visualize for Github notebook rendering\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.imshow(np.squeeze(np.append(img[:,:,0], blurred_img[:,:,0], axis=1)), \n",
    "           cmap='gray', vmin=0, vmax=1)\n",
    "plt.colorbar()\n",
    "\n",
    "# Visuzlize using this on local computer. Commenting this because Github \n",
    "# does not show ipywidgets\n",
    "# imshow3D(np.squeeze(np.append(img, blurred_img, axis=2)), 0, (12,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image intensity partial sensor range coverage\n",
    "\n",
    "Moreover the image intensity does not cover the entire spread of the avaiable dynamic range in the sensor. We simulate this over here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_range = {'lower':0.2, 'upper':0.2, 'gap':0.6}\n",
    "\n",
    "lower = np.random.random()*im_range['lower']\n",
    "upper = np.random.random()*im_range['upper'] + im_range['gap']\n",
    "range_img = imgen.change_im_range(blurred_img, lower, upper)\n",
    "\n",
    "# Show the two images side-by side to visualize the original image (left) vs \n",
    "# the cropped sensor range image intensity coverage(right)\n",
    "\n",
    "# Temporary visualize for Github notebook rendering\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.imshow(np.squeeze(np.append(img[:,:,0], range_img[:,:,0], axis=1)), \n",
    "           cmap='gray', vmin=0, vmax=1)\n",
    "plt.colorbar()\n",
    "\n",
    "# Visuzlize using this on local computer. Commenting this because Github \n",
    "# does not show ipywidgets\n",
    "\n",
    "# imshow3D(np.squeeze(np.append(img, range_img, axis=2)), 0, (12,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensor shot noise\n",
    "\n",
    "To generate the simulated images for each signal-to-noise ratio (SNr) from shot noise, we scale the image intensity such that the peak image intensity is equal to the SNr squared. Then each voxel intensity is replaced by a random number drawn from a Poissonâ€™s distribution with its mean equal to the original intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr = 12\n",
    "noise_img = imgen.add_poisson_noise(range_img, snr)\n",
    "\n",
    "# Show the two images side-by side to visualize the original image (left) vs \n",
    "# image with sensor shot noise(right)\n",
    "\n",
    "# Temporary visualize for Github notebook rendering\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.imshow(np.squeeze(np.append(img[:,:,0], noise_img[:,:,0], axis=1)), \n",
    "           cmap='gray', vmin=0, vmax=1)\n",
    "plt.colorbar()\n",
    "\n",
    "\n",
    "# Visuzlize using this on local computer. Commenting this because Github \n",
    "# does not show ipywidgets\n",
    "\n",
    "# imshow3D(np.squeeze(np.append(img, noise_img, axis=2)), 0, (12,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian sensor noise\n",
    "\n",
    "At last, we add gaussian noise to the image to simulate the usual gaussian noise present in any image. This is the last step in our synthetic noisy image generation. We can see how much different the noisy images are in comparison to the original images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_noise = {'mean':0, 'std':0.02}\n",
    "noise_img = imgen.add_gaussian_noise(noise_img, gauss_noise['mean'], gauss_noise['std'])\n",
    "\n",
    "# Show the two images side-by side to visualize the original image (left) vs \n",
    "# image with gaussian sensor noise(right)\n",
    "\n",
    "# Temporary visualize for Github notebook rendering\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.imshow(np.squeeze(np.append(img[:,:,0], noise_img[:,:,0], axis=1)), \n",
    "           cmap='gray', vmin=0, vmax=1)\n",
    "plt.colorbar()\n",
    "\n",
    "# Visuzlize using this on local computer. Commenting this because Github \n",
    "# does not show ipywidgets\n",
    "\n",
    "# imshow3D(np.squeeze(np.append(img, noise_img, axis=2)), 0, (12,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic dataset generation for training and test the model\n",
    "\n",
    "Caution: This step takes time, so run it with some patience. On my computer it took about 13 seconds to generate 1000 images. \n",
    "\n",
    "I generate 27000 training images with 3000 test/validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image generation parameters\n",
    "n_images = 100\n",
    "slices = [x for x in range(14, 114, 5)]\n",
    "sizeI = np.array([64,64,128])\n",
    "sigmaPts = {280:1.5*np.array([1,1,1]), 140:1.75*np.array([1,1,1]), 70:2*np.array([1,1,1])}\n",
    "im_range = {'lower':0.2, 'upper':0.2, 'gap':0.6}\n",
    "gauss_noise = {'mean':0, 'std':0.01}\n",
    "snr = 12\n",
    "psf = sio.loadmat('../data/psf.mat')\n",
    "psf = psf['psf']\n",
    "psf = psf/np.sum(psf)\n",
    "\n",
    "# Generate Images\n",
    "start = time.time()\n",
    "X_train, y_train = imgen.create_im_2D(n_images, sizeI, slices, sigmaPts, psf,snr=snr, \n",
    "                            im_range=im_range, gauss_noise=gauss_noise)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.10)\n",
    "end = time.time()\n",
    "print('Time elapsed in data generation: %0.3f seconds' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize synthetically generated images. Clean on left vs noisy on right\n",
    "\n",
    "# Temporary visualize for Github notebook rendering\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.imshow(np.squeeze(np.append(X_test[0,:,:,:], y_test[0,:,:,:], axis=1)), \n",
    "           cmap='gray', vmin=0, vmax=1)\n",
    "plt.colorbar()\n",
    "\n",
    "# Visuzlize using this on local computer. Commenting this because Github \n",
    "# does not show ipywidgets\n",
    "# imshow3D(np.squeeze(np.append(X_train[:30,:,:,:], y_train[:30,:,:,:], axis=2)),\n",
    "#          0, (12,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional autoencoder model\n",
    "\n",
    "I use a convolutional autoencoder model to recover back the original image from the noisy image. The model architecture can be seen in the figure below\n",
    "\n",
    "\n",
    "I trained various models, and the model shown above gives the best accuracy scores. In the this model, I use three layers of convolution with relu activation to reduce the feature size from 64x64x1 to 8x8x45. Then I use four layers of transposed convolution with relue activatation to increase the feature size from 8x8x45 to 128x128x10. The I finally use a convolution layer coupled with sigmoid activation to recover the cleaned image. \n",
    "\n",
    "I minize the cross-entropy loss in training using Adam optimizer to train the model. \n",
    "\n",
    "#### Evaluation parameter\n",
    "In the model I define accuracy in terms of error. Here error is defined as the mean of absolute image intensity difference between noisy and the true clean image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is implemented in model.py with Keras like api to fit, evaluate, predict, save and restore.\n",
    "# This allows for clean training process over here. The model is however built using Tensorflow, and\n",
    "# implemented with Keras/scikit-learn like api\n",
    "\n",
    "model = ImageCleaner()\n",
    "model.lr = 0.001\n",
    "model.dropout = 0.1\n",
    "\n",
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=200, \n",
    "          val_x=X_test, \n",
    "          val_y=y_test,\n",
    "          save_path='./checkpoints/mymodel')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize training loss and error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,2,figsize=(15,5));\n",
    "\n",
    "axs[0].plot(model.train_epoch, model.train_loss, 'b', label='Training')\n",
    "axs[0].plot(model.train_epoch, model.val_loss, 'g', label='Validation')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('Loss vs training epochs')\n",
    "axs[0].set_xlabel('Number of epochs')\n",
    "axs[0].set_ylabel('Loss')\n",
    "\n",
    "axs[1].plot(model.train_epoch, model.train_error, 'b', label='Training')\n",
    "axs[1].plot(model.train_epoch, model.val_error, 'g', label='Validation')\n",
    "axs[1].legend()\n",
    "axs[1].set_title('Error vs training epochs')\n",
    "axs[1].set_xlabel('Number of epochs')\n",
    "axs[1].set_ylabel('Accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model for future\n",
    "\n",
    "Save model for future use and restore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './final_model/my_model.ckpt'\n",
    "model.save(save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './final_model/my_model.ckpt'\n",
    "\n",
    "model = ImageCleaner()\n",
    "model.restore(save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Lets visually look at some of the results and see how well did the model recover the orignial image from the noisy image. \n",
    "\n",
    "We can see that the results is quite remarkable. Model learnt pretty well how to denoise the synthetic images. The images are almost restored to their original self. Once in an while there is a very small difference, but it is honestly quite neglible. I'm very impressed with how well the model can recover images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = sklearn.utils.shuffle(X_test, y_test)\n",
    "X_predict = model.predict(X_test, batch_size = 10)\n",
    "\n",
    "n = 4\n",
    "canvas = np.empty((64 * n, 64*3))\n",
    "for i in range(n):\n",
    "    # Draw the generated digits\n",
    "    canvas[i*64 : (i+1)*64, 0:64] = np.squeeze(X_test[i,:,:,:])\n",
    "    canvas[i*64 : (i+1)*64, 64:128] = np.squeeze(X_predict[i,:,:,:])\n",
    "    canvas[i*64 : (i+1)*64, 128:192] = np.squeeze(y_test[i,:,:,:])\n",
    "\n",
    "print(\"Noisy images (left), Denoised images from model (center), original images (right)\")     \n",
    "plt.figure(figsize=(15,5*n))\n",
    "plt.imshow(canvas, cmap='gray', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We can see that convolutional auto-encoders are very powerful and can have significant uses in image cleaning. More sophisticated models can be built by training with real micrscope images, such that such techniques can be used in wild to deconvolve and clean microscope images. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
